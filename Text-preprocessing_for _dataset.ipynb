{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In any machine learning task, cleaning or preprocessing the data is as important as model building if not more. And when it comes to unstructured data like text, this process is even more important. \n",
    "\n",
    "Objective of this kernel is to understand the various text preprocessing steps with code examples. \n",
    "\n",
    "Some of the common text preprocessing / cleaning steps are:\n",
    "* Lower casing\n",
    "* Removal of Punctuations\n",
    "* Removal of Stopwords\n",
    "* Removal of Frequent words\n",
    "* Removal of Rare words\n",
    "* Stemming\n",
    "* Lemmatization\n",
    "* Removal of emojis\n",
    "* Removal of emoticons\n",
    "* Conversion of emoticons to words\n",
    "* Conversion of emojis to words\n",
    "* Removal of URLs \n",
    "* Removal of HTML tags\n",
    "* Chat words conversion\n",
    "* Spelling correction\n",
    "\n",
    "\n",
    "So these are the different types of text preprocessing steps which we can do on text data. But we need not do all of these all the times. We need to carefully choose the preprocessing steps based on our use case since that also play an important role. \n",
    "\n",
    "For example, in sentiment analysis use case, we need not remove the emojis or emoticons as it will convey some important information about the sentiment. Similarly we need to decide based on our use cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "import string\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>directory</th>\n",
       "      <th>category</th>\n",
       "      <th>fileName</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>C:/Users/DELL PC/Desktop/Document Clustering U...</td>\n",
       "      <td>DATASET\\business</td>\n",
       "      <td>business_1.txt</td>\n",
       "      <td>Lufthansa flies back to profit</td>\n",
       "      <td>['German airline Lufthansa has returned to pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>C:/Users/DELL PC/Desktop/Document Clustering U...</td>\n",
       "      <td>DATASET\\business</td>\n",
       "      <td>business_10.txt</td>\n",
       "      <td>Winn-Dixie files for bankruptcy</td>\n",
       "      <td>['US supermarket group Winn-Dixie has filed fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>C:/Users/DELL PC/Desktop/Document Clustering U...</td>\n",
       "      <td>DATASET\\business</td>\n",
       "      <td>business_100.txt</td>\n",
       "      <td>US economy still growing says Fed</td>\n",
       "      <td>['Most areas of the US saw their economy conti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>C:/Users/DELL PC/Desktop/Document Clustering U...</td>\n",
       "      <td>DATASET\\business</td>\n",
       "      <td>business_11.txt</td>\n",
       "      <td>Saab to build Cadillacs in Sweden</td>\n",
       "      <td>[\"General Motors, the world's largest car make...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>C:/Users/DELL PC/Desktop/Document Clustering U...</td>\n",
       "      <td>DATASET\\business</td>\n",
       "      <td>business_12.txt</td>\n",
       "      <td>Bank voted 8-1 for no rate change</td>\n",
       "      <td>[\"The decision to keep interest rates on hold ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                          directory  \\\n",
       "0           0  C:/Users/DELL PC/Desktop/Document Clustering U...   \n",
       "1           1  C:/Users/DELL PC/Desktop/Document Clustering U...   \n",
       "2           2  C:/Users/DELL PC/Desktop/Document Clustering U...   \n",
       "3           3  C:/Users/DELL PC/Desktop/Document Clustering U...   \n",
       "4           4  C:/Users/DELL PC/Desktop/Document Clustering U...   \n",
       "\n",
       "           category          fileName                              title  \\\n",
       "0  DATASET\\business    business_1.txt     Lufthansa flies back to profit   \n",
       "1  DATASET\\business   business_10.txt    Winn-Dixie files for bankruptcy   \n",
       "2  DATASET\\business  business_100.txt  US economy still growing says Fed   \n",
       "3  DATASET\\business   business_11.txt  Saab to build Cadillacs in Sweden   \n",
       "4  DATASET\\business   business_12.txt  Bank voted 8-1 for no rate change   \n",
       "\n",
       "                                                text  \n",
       "0  ['German airline Lufthansa has returned to pro...  \n",
       "1  ['US supermarket group Winn-Dixie has filed fo...  \n",
       "2  ['Most areas of the US saw their economy conti...  \n",
       "3  [\"General Motors, the world's largest car make...  \n",
       "4  [\"The decision to keep interest rates on hold ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df = pd.read_csv(\"Jeasn_baxter_final.csv\", encoding= 'utf-8')\n",
    "df = full_df[[\"text\"]]\n",
    "df[\"ABC\"] = df[\"text\"].astype(str)\n",
    "#df[\"ABX\"] = df[\"ABX\"].astype(str)\n",
    "full_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df['category'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "source": [
    "## Lower Casing\n",
    "\n",
    "Lower casing is a common text preprocessing technique. The idea is to convert the input text into same casing format so that 'text', 'Text' and 'TEXT' are treated the same way. \n",
    "\n",
    "This is more helpful for text featurization techniques like frequency, tfidf as it helps to combine the same words together thereby reducing the duplication and get correct counts / tfidf values.\n",
    "\n",
    "This may not be helpful when we do tasks like Part of Speech tagging (where proper casing gives some information about Nouns and so on) and Sentiment Analysis (where upper casing refers to anger and so on)\n",
    "\n",
    "By default, lower casing is done my most of the modern day vecotirzers and tokenizers like [sklearn TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) and [Keras Tokenizer](https://keras.io/preprocessing/text/). So we need to set them to false as needed depending on our use case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>ABC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['German airline Lufthansa has returned to pro...</td>\n",
       "      <td>['german airline lufthansa has returned to pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['US supermarket group Winn-Dixie has filed fo...</td>\n",
       "      <td>['us supermarket group winn-dixie has filed fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['Most areas of the US saw their economy conti...</td>\n",
       "      <td>['most areas of the us saw their economy conti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[\"General Motors, the world's largest car make...</td>\n",
       "      <td>[\"general motors, the world's largest car make...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[\"The decision to keep interest rates on hold ...</td>\n",
       "      <td>[\"the decision to keep interest rates on hold ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  ['German airline Lufthansa has returned to pro...   \n",
       "1  ['US supermarket group Winn-Dixie has filed fo...   \n",
       "2  ['Most areas of the US saw their economy conti...   \n",
       "3  [\"General Motors, the world's largest car make...   \n",
       "4  [\"The decision to keep interest rates on hold ...   \n",
       "\n",
       "                                                 ABC  \n",
       "0  ['german airline lufthansa has returned to pro...  \n",
       "1  ['us supermarket group winn-dixie has filed fo...  \n",
       "2  ['most areas of the us saw their economy conti...  \n",
       "3  [\"general motors, the world's largest car make...  \n",
       "4  [\"the decision to keep interest rates on hold ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"ABC\"] = df[\"ABC\"].str.lower()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_pattern.sub(r'', text)\n",
    "\n",
    "for i in range(0,len(df[\"ABC\"])):\n",
    "    df[\"ABC\"][i]=remove_urls(df[\"ABC\"][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def remove_html(text):\n",
    "    return BeautifulSoup(text, \"lxml\").text\n",
    "\n",
    "for i in range(0,len(df[\"ABC\"])):\n",
    "    df[\"ABC\"][i]=remove_html(df[\"ABC\"][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker\n",
    "\n",
    "spell = SpellChecker()\n",
    "def correct_spellings(text):\n",
    "    corrected_text = []\n",
    "    misspelled_words = spell.unknown(text.split())\n",
    "    for word in text.split():\n",
    "        if word in misspelled_words:\n",
    "            corrected_text.append(spell.correction(word))\n",
    "        else:\n",
    "            corrected_text.append(word)\n",
    "    return \" \".join(corrected_text)\n",
    "        \n",
    "for i in range(0,len(df[\"ABC\"])):\n",
    "    df[\"ABC\"][i]=correct_spellings(df[\"ABC\"][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removal of Punctuations\n",
    "\n",
    "One another common text preprocessing technique is to remove the punctuations from the text data. This is again a text standardization process that will help to treat 'hurray' and 'hurray!' in the same way.\n",
    "\n",
    "We also need to carefully choose the list of punctuations to exclude depending on the use case. For example, the `string.punctuation` in python contains the following punctuation symbols \n",
    "\n",
    "`!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~`\n",
    "\n",
    "We can add or remove more punctuations as per our need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>ABC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['German airline Lufthansa has returned to pro...</td>\n",
       "      <td>german airline lufthansa has returned to profi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['US supermarket group Winn-Dixie has filed fo...</td>\n",
       "      <td>us supermarket group winndixie has filed for b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['Most areas of the US saw their economy conti...</td>\n",
       "      <td>most areas of the us saw their economy continu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[\"General Motors, the world's largest car make...</td>\n",
       "      <td>general motors the worlds largest car maker ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[\"The decision to keep interest rates on hold ...</td>\n",
       "      <td>the decision to keep interest rates on hold at...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  ['German airline Lufthansa has returned to pro...   \n",
       "1  ['US supermarket group Winn-Dixie has filed fo...   \n",
       "2  ['Most areas of the US saw their economy conti...   \n",
       "3  [\"General Motors, the world's largest car make...   \n",
       "4  [\"The decision to keep interest rates on hold ...   \n",
       "\n",
       "                                                 ABC  \n",
       "0  german airline lufthansa has returned to profi...  \n",
       "1  us supermarket group winndixie has filed for b...  \n",
       "2  most areas of the us saw their economy continu...  \n",
       "3  general motors the worlds largest car maker ha...  \n",
       "4  the decision to keep interest rates on hold at...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop the new column created in last cell\n",
    "#df.drop([\"text_lower\"], axis=1, inplace=True)\n",
    "\n",
    "PUNCT_TO_REMOVE = string.punctuation\n",
    "def remove_punctuation(text):\n",
    "    \"\"\"custom function to remove the punctuation\"\"\"\n",
    "    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n",
    "\n",
    "df[\"ABC\"] = df[\"ABC\"].apply(lambda text: remove_punctuation(text))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removal of stopwords\n",
    "\n",
    "Stopwords are commonly occuring words in a language like 'the', 'a' and so on. They can be removed from the text most of the times, as they don't provide valuable information for downstream analysis. In cases like Part of Speech tagging, we should not remove them as provide very valuable information about the POS.\n",
    "\n",
    "These stopword lists are already compiled for different languages and we can safely use them. For example, the stopword list for english language from the nltk package can be seen below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"i, me, my, myself, we, our, ours, ourselves, you, you're, you've, you'll, you'd, your, yours, yourself, yourselves, he, him, his, himself, she, she's, her, hers, herself, it, it's, its, itself, they, them, their, theirs, themselves, what, which, who, whom, this, that, that'll, these, those, am, is, are, was, were, be, been, being, have, has, had, having, do, does, did, doing, a, an, the, and, but, if, or, because, as, until, while, of, at, by, for, with, about, against, between, into, through, during, before, after, above, below, to, from, up, down, in, out, on, off, over, under, again, further, then, once, here, there, when, where, why, how, all, any, both, each, few, more, most, other, some, such, no, nor, not, only, own, same, so, than, too, very, s, t, can, will, just, don, don't, should, should've, now, d, ll, m, o, re, ve, y, ain, aren, aren't, couldn, couldn't, didn, didn't, doesn, doesn't, hadn, hadn't, hasn, hasn't, haven, haven't, isn, isn't, ma, mightn, mightn't, mustn, mustn't, needn, needn't, shan, shan't, shouldn, shouldn't, wasn, wasn't, weren, weren't, won, won't, wouldn, wouldn't\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\", \".join(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly we can also get the list for other languages as well and use them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>ABC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['German airline Lufthansa has returned to pro...</td>\n",
       "      <td>german airline lufthansa returned profit 2004 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['US supermarket group Winn-Dixie has filed fo...</td>\n",
       "      <td>us supermarket group winndixie filed bankruptc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['Most areas of the US saw their economy conti...</td>\n",
       "      <td>areas us saw economy continue expand december ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[\"General Motors, the world's largest car make...</td>\n",
       "      <td>general motors worlds largest car maker confir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[\"The decision to keep interest rates on hold ...</td>\n",
       "      <td>decision keep interest rates hold 475 earlier ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  ['German airline Lufthansa has returned to pro...   \n",
       "1  ['US supermarket group Winn-Dixie has filed fo...   \n",
       "2  ['Most areas of the US saw their economy conti...   \n",
       "3  [\"General Motors, the world's largest car make...   \n",
       "4  [\"The decision to keep interest rates on hold ...   \n",
       "\n",
       "                                                 ABC  \n",
       "0  german airline lufthansa returned profit 2004 ...  \n",
       "1  us supermarket group winndixie filed bankruptc...  \n",
       "2  areas us saw economy continue expand december ...  \n",
       "3  general motors worlds largest car maker confir...  \n",
       "4  decision keep interest rates hold 475 earlier ...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STOPWORDS = set(stopwords.words('english'))\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"custom function to remove the stopwords\"\"\"\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "\n",
    "df[\"ABC\"] = df[\"ABC\"].apply(lambda text: remove_stopwords(text))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removal of Frequent words\n",
    "\n",
    "In the previos preprocessing step, we removed the stopwords based on language information. But say, if we have a domain specific corpus, we might also have some frequent words which are of not so much importance to us. \n",
    "\n",
    "So this step is to remove the frequent words in the given corpus. If we use something like tfidf, this is automatically taken care of.  \n",
    "\n",
    "Let us get the most common words adn then remove them in the next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('said', 1673),\n",
       " ('also', 957),\n",
       " ('would', 947),\n",
       " ('one', 805),\n",
       " ('new', 681),\n",
       " ('mr', 643),\n",
       " ('war', 633),\n",
       " ('us', 615),\n",
       " ('people', 593),\n",
       " ('first', 579)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "cnt = Counter()\n",
    "for text in df[\"ABC\"].values:\n",
    "    for word in text.split():\n",
    "        cnt[word] += 1\n",
    "        \n",
    "cnt.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>ABC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['German airline Lufthansa has returned to pro...</td>\n",
       "      <td>german airline lufthansa returned profit 2004 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['US supermarket group Winn-Dixie has filed fo...</td>\n",
       "      <td>supermarket group winndixie filed bankruptcy p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['Most areas of the US saw their economy conti...</td>\n",
       "      <td>areas saw economy continue expand december ear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[\"General Motors, the world's largest car make...</td>\n",
       "      <td>general motors worlds largest car maker confir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[\"The decision to keep interest rates on hold ...</td>\n",
       "      <td>decision keep interest rates hold 475 earlier ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  ['German airline Lufthansa has returned to pro...   \n",
       "1  ['US supermarket group Winn-Dixie has filed fo...   \n",
       "2  ['Most areas of the US saw their economy conti...   \n",
       "3  [\"General Motors, the world's largest car make...   \n",
       "4  [\"The decision to keep interest rates on hold ...   \n",
       "\n",
       "                                                 ABC  \n",
       "0  german airline lufthansa returned profit 2004 ...  \n",
       "1  supermarket group winndixie filed bankruptcy p...  \n",
       "2  areas saw economy continue expand december ear...  \n",
       "3  general motors worlds largest car maker confir...  \n",
       "4  decision keep interest rates hold 475 earlier ...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FREQWORDS = set([w for (w, wc) in cnt.most_common(10)])\n",
    "def remove_freqwords(text):\n",
    "    \"\"\"custom function to remove the frequent words\"\"\"\n",
    "    return \" \".join([word for word in str(text).split() if word not in FREQWORDS])\n",
    "\n",
    "df[\"ABC\"] = df[\"ABC\"].apply(lambda text: remove_freqwords(text))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removal of Rare words\n",
    "\n",
    "This is very similar to previous preprocessing step but we will remove the rare words from the corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>ABC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['German airline Lufthansa has returned to pro...</td>\n",
       "      <td>german airline lufthansa returned profit 2004 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['US supermarket group Winn-Dixie has filed fo...</td>\n",
       "      <td>supermarket group winndixie filed bankruptcy p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['Most areas of the US saw their economy conti...</td>\n",
       "      <td>areas saw economy continue expand december ear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[\"General Motors, the world's largest car make...</td>\n",
       "      <td>general motors worlds largest car maker confir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[\"The decision to keep interest rates on hold ...</td>\n",
       "      <td>decision keep interest rates hold 475 earlier ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  ['German airline Lufthansa has returned to pro...   \n",
       "1  ['US supermarket group Winn-Dixie has filed fo...   \n",
       "2  ['Most areas of the US saw their economy conti...   \n",
       "3  [\"General Motors, the world's largest car make...   \n",
       "4  [\"The decision to keep interest rates on hold ...   \n",
       "\n",
       "                                                 ABC  \n",
       "0  german airline lufthansa returned profit 2004 ...  \n",
       "1  supermarket group winndixie filed bankruptcy p...  \n",
       "2  areas saw economy continue expand december ear...  \n",
       "3  general motors worlds largest car maker confir...  \n",
       "4  decision keep interest rates hold 475 earlier ...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop the two columns which are no more needed \n",
    "#df.drop([\"text_wo_punct\", \"text_wo_stop\"], axis=1, inplace=True)\n",
    "\n",
    "n_rare_words = 10\n",
    "RAREWORDS = set([w for (w, wc) in cnt.most_common()[:-n_rare_words-1:-1]])\n",
    "def remove_rarewords(text):\n",
    "    \"\"\"custom function to remove the rare words\"\"\"\n",
    "    return \" \".join([word for word in str(text).split() if word not in RAREWORDS])\n",
    "\n",
    "df[\"ABC\"] = df[\"ABC\"].apply(lambda text: remove_rarewords(text))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can combine all the list of words (stopwords, frequent words and rare words) and create a single list to remove them at once.\n",
    "\n",
    "## Stemming\n",
    "\n",
    "Stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form (From [Wikipedia](https://en.wikipedia.org/wiki/Stemming))\n",
    "\n",
    "For example, if there are two words in the corpus `walks` and `walking`, then stemming will stem the suffix to make them `walk`. But say in another example, we have two words `console` and `consoling`, the stemmer will remove the suffix and make them `consol` which is not a proper english word.\n",
    "\n",
    "There are several type of stemming algorithms available and one of the famous one is porter stemmer which is widely used. We can use nltk package for the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>ABC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['German airline Lufthansa has returned to pro...</td>\n",
       "      <td>german airlin lufthansa return profit 2004 pos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['US supermarket group Winn-Dixie has filed fo...</td>\n",
       "      <td>supermarket group winndixi file bankruptci pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['Most areas of the US saw their economy conti...</td>\n",
       "      <td>area saw economi continu expand decemb earli j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[\"General Motors, the world's largest car make...</td>\n",
       "      <td>gener motor world largest car maker confirm bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[\"The decision to keep interest rates on hold ...</td>\n",
       "      <td>decis keep interest rate hold 475 earlier mont...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  ['German airline Lufthansa has returned to pro...   \n",
       "1  ['US supermarket group Winn-Dixie has filed fo...   \n",
       "2  ['Most areas of the US saw their economy conti...   \n",
       "3  [\"General Motors, the world's largest car make...   \n",
       "4  [\"The decision to keep interest rates on hold ...   \n",
       "\n",
       "                                                 ABC  \n",
       "0  german airlin lufthansa return profit 2004 pos...  \n",
       "1  supermarket group winndixi file bankruptci pro...  \n",
       "2  area saw economi continu expand decemb earli j...  \n",
       "3  gener motor world largest car maker confirm bu...  \n",
       "4  decis keep interest rate hold 475 earlier mont...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Drop the two columns \n",
    "#df.drop([\"text_wo_stopfreq\", \"text_wo_stopfreqrare\"], axis=1, inplace=True) \n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "def stem_words(text):\n",
    "    return \" \".join([stemmer.stem(word) for word in text.split()])\n",
    "\n",
    "df[\"ABC\"] = df[\"ABC\"].apply(lambda text: stem_words(text))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that words like `private` and `propose` have their `e` at the end chopped off due to stemming. This is not intented. What can we do fort hat? We can use Lemmatization in such cases.\n",
    "\n",
    "Also this porter stemmer is for English language. If we are working with other languages, we can use snowball stemmer. The supported languages for snowball stemmer are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('arabic',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'hungarian',\n",
       " 'italian',\n",
       " 'norwegian',\n",
       " 'porter',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'spanish',\n",
       " 'swedish')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "SnowballStemmer.languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "\n",
    "Lemmatization is similar to stemming in reducing inflected words to their word stem but differs in the way that it makes sure the root word (also called as lemma) belongs to the language. \n",
    "\n",
    "As a result, this one is generally slower than stemming process. So depending on the speed requirement, we can choose to use either stemming or lemmatization. \n",
    "\n",
    "Let us use the `WordNetLemmatizer` in nltk to lemmatize our sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>ABC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['German airline Lufthansa has returned to pro...</td>\n",
       "      <td>german airlin lufthansa return profit 2004 pos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['US supermarket group Winn-Dixie has filed fo...</td>\n",
       "      <td>supermarket group winndixi file bankruptci pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['Most areas of the US saw their economy conti...</td>\n",
       "      <td>area saw economi continu expand decemb earli j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[\"General Motors, the world's largest car make...</td>\n",
       "      <td>gener motor world largest car maker confirm bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[\"The decision to keep interest rates on hold ...</td>\n",
       "      <td>decis keep interest rate hold 475 earlier mont...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  ['German airline Lufthansa has returned to pro...   \n",
       "1  ['US supermarket group Winn-Dixie has filed fo...   \n",
       "2  ['Most areas of the US saw their economy conti...   \n",
       "3  [\"General Motors, the world's largest car make...   \n",
       "4  [\"The decision to keep interest rates on hold ...   \n",
       "\n",
       "                                                 ABC  \n",
       "0  german airlin lufthansa return profit 2004 pos...  \n",
       "1  supermarket group winndixi file bankruptci pro...  \n",
       "2  area saw economi continu expand decemb earli j...  \n",
       "3  gener motor world largest car maker confirm bu...  \n",
       "4  decis keep interest rate hold 475 earlier mont...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_words(text):\n",
    "    return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "\n",
    "df[\"ABC\"] = df[\"ABC\"].apply(lambda text: lemmatize_words(text))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the trailing `e` in the `propose` and `private` is retained when we use lemmatization unlike stemming. \n",
    "\n",
    "Wait. There is one more thing in lemmatization. Let us try to lemmatize `running` now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'running'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"running\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow. It returned `running` as such without converting it to the root form `run`. This is because the lemmatization process depends on the POS tag to come up with the correct lemma. Now let us lemmatize again by providing the POS tag for the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"running\", \"v\") # v for verb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are getting the root form `run`. So we also need to provide the POS tag of the word along with the word for lemmatizer in nltk. Depending on the POS, the lemmatizer may return different results.\n",
    "\n",
    "Let us take the example, `stripes` and check the lemma when it is both verb and noun."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us redo the lemmatization process for our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>ABC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['German airline Lufthansa has returned to pro...</td>\n",
       "      <td>german airlin lufthansa return profit 2004 pos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['US supermarket group Winn-Dixie has filed fo...</td>\n",
       "      <td>supermarket group winndixi file bankruptci pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['Most areas of the US saw their economy conti...</td>\n",
       "      <td>area saw economi continu expand decemb earli j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[\"General Motors, the world's largest car make...</td>\n",
       "      <td>gener motor world large car maker confirm buil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[\"The decision to keep interest rates on hold ...</td>\n",
       "      <td>decis keep interest rate hold 475 early month ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  ['German airline Lufthansa has returned to pro...   \n",
       "1  ['US supermarket group Winn-Dixie has filed fo...   \n",
       "2  ['Most areas of the US saw their economy conti...   \n",
       "3  [\"General Motors, the world's largest car make...   \n",
       "4  [\"The decision to keep interest rates on hold ...   \n",
       "\n",
       "                                                 ABC  \n",
       "0  german airlin lufthansa return profit 2004 pos...  \n",
       "1  supermarket group winndixi file bankruptci pro...  \n",
       "2  area saw economi continu expand decemb earli j...  \n",
       "3  gener motor world large car maker confirm buil...  \n",
       "4  decis keep interest rate hold 475 early month ...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "wordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\n",
    "def lemmatize_words(text):\n",
    "    pos_tagged_text = nltk.pos_tag(text.split())\n",
    "    return \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\n",
    "\n",
    "df[\"ABC\"] = df[\"ABC\"].apply(lambda text: lemmatize_words(text))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removal of URLs\n",
    "\n",
    "Next preprocessing step is to remove any URLs present in the data. For example, if we are doing a twitter analysis, then there is a good chance that the tweet will have some URL in it. Probably we might need to remove them for our further analysis. \n",
    "\n",
    "We can use the below code snippet to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us take a `https` link and check the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us take a `http` url and check the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removal of HTML Tags\n",
    "\n",
    "One another common preprocessing technique that will come handy in multiple places is removal of html tags. This is especially useful, if we scrap the data from different websites. We might end up having html strings as part of our text. \n",
    "\n",
    "First, let us try to remove the HTML tags using regular expressions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use `BeautifulSoup` package to get the text from HTML document in a more elegant way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Words Conversion\n",
    "\n",
    "This is an important text preprocessing step if we are dealing with chat data. People do use a lot of abbreviated words in chat and so it might be helpful to expand those words for our analysis purposes. \n",
    "\n",
    "Got a good list of chat slang words from this [repo](https://github.com/rishabhverma17/sms_slang_translator/blob/master/slang.txt). We can use this for our conversion here. We can add more words to this list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "chat_words_str = \"\"\"\n",
    "AFAIK=As Far As I Know\n",
    "AFK=Away From Keyboard\n",
    "ASAP=As Soon As Possible\n",
    "ATK=At The Keyboard\n",
    "ATM=At The Moment\n",
    "A3=Anytime, Anywhere, Anyplace\n",
    "BAK=Back At Keyboard\n",
    "BBL=Be Back Later\n",
    "BBS=Be Back Soon\n",
    "BFN=Bye For Now\n",
    "B4N=Bye For Now\n",
    "BRB=Be Right Back\n",
    "BRT=Be Right There\n",
    "BTW=By The Way\n",
    "B4=Before\n",
    "B4N=Bye For Now\n",
    "CU=See You\n",
    "CUL8R=See You Later\n",
    "CYA=See You\n",
    "FAQ=Frequently Asked Questions\n",
    "FC=Fingers Crossed\n",
    "FWIW=For What It's Worth\n",
    "FYI=For Your Information\n",
    "GAL=Get A Life\n",
    "GG=Good Game\n",
    "GN=Good Night\n",
    "GMTA=Great Minds Think Alike\n",
    "GR8=Great!\n",
    "G9=Genius\n",
    "IC=I See\n",
    "ICQ=I Seek you (also a chat program)\n",
    "ILU=ILU: I Love You\n",
    "IMHO=In My Honest/Humble Opinion\n",
    "IMO=In My Opinion\n",
    "IOW=In Other Words\n",
    "IRL=In Real Life\n",
    "KISS=Keep It Simple, Stupid\n",
    "LDR=Long Distance Relationship\n",
    "LMAO=Laugh My A.. Off\n",
    "LOL=Laughing Out Loud\n",
    "LTNS=Long Time No See\n",
    "L8R=Later\n",
    "MTE=My Thoughts Exactly\n",
    "M8=Mate\n",
    "NRN=No Reply Necessary\n",
    "OIC=Oh I See\n",
    "PITA=Pain In The A..\n",
    "PRT=Party\n",
    "PRW=Parents Are Watching\n",
    "ROFL=Rolling On The Floor Laughing\n",
    "ROFLOL=Rolling On The Floor Laughing Out Loud\n",
    "ROTFLMAO=Rolling On The Floor Laughing My A.. Off\n",
    "SK8=Skate\n",
    "STATS=Your sex and age\n",
    "ASL=Age, Sex, Location\n",
    "THX=Thank You\n",
    "TTFN=Ta-Ta For Now!\n",
    "TTYL=Talk To You Later\n",
    "U=You\n",
    "U2=You Too\n",
    "U4E=Yours For Ever\n",
    "WB=Welcome Back\n",
    "WTF=What The F...\n",
    "WTG=Way To Go!\n",
    "WUF=Where Are You From?\n",
    "W8=Wait...\n",
    "7K=Sick:-D Laugher\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_words_map_dict = {}\n",
    "chat_words_list = []\n",
    "for line in chat_words_str.split(\"\\n\"):\n",
    "    if line != \"\":\n",
    "        cw = line.split(\"=\")[0]\n",
    "        cw_expanded = line.split(\"=\")[1]\n",
    "        chat_words_list.append(cw)\n",
    "        chat_words_map_dict[cw] = cw_expanded\n",
    "chat_words_list = set(chat_words_list)\n",
    "\n",
    "def chat_words_conversion(text):\n",
    "    new_text = []\n",
    "    for w in text.split():\n",
    "        if w.upper() in chat_words_list:\n",
    "            new_text.append(chat_words_map_dict[w.upper()])\n",
    "        else:\n",
    "            new_text.append(w)\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "for i in range(0,len(df[\"ABC\"])):\n",
    "    df[\"ABC\"][i]=chat_words_conversion(df[\"ABC\"][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "growth japan evapor three month septemb spark renew concern economi long decadelong trough output period grow 01 annual rate 03 export usual engin recoveri falter domest demand stay subdu corpor invest fell short growth fall well short expect mark sixth straight quarter expans economi stagnat throughout 1990 experienc brief spurt expans amid long period doldrum result deflat price fall rather rise make japanes shopper cautiou keep spend effect leav economi depend ever export recent recoveri high oil price knock 02 growth rate fall dollar mean product ship becom rel expens perform third quarter mark sharp downturn earlier year quarter show annual growth 63 second show 11 economist predict much 2 time around export slow capit spend becam weaker hiromichi shirakawa chief economist ub secur tokyo person consumpt look good mainli due temporari factor olymp amber light flash govern may find difficult rais tax polici implement economi pick help deal japan massiv public debt\n"
     ]
    }
   ],
   "source": [
    "#chat_words_conversion(\"imo this is awesome\")\n",
    "print(df['ABC'][12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=[]\n",
    "x=0\n",
    "for i in range(0,len(df[\"ABC\"])):\n",
    "    a=[]\n",
    "    a.append(x)\n",
    "    x=x+1\n",
    "    a.append(df[\"ABC\"][i])\n",
    "    a.append(df[\"text\"][i])\n",
    "    a.append(full_df['category'][i])\n",
    "    l.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 'close associ former yuko bos mikhail khodorkovski tell court fraud charg level fals platon lebedev trial alongsid khodorkovski sinc june case centr around privatis fertilis firm pair claim punish author polit ambit khodorkovski lebedev absurd contradict case open defenc could see legal basi charg face includ alleg tax evas embarrass could understand file complaint tell moscow court lebedev head menatep group parent compani yuko lebedev khodorkovski face possibl 10 year jail sentenc convict question judg next day khodorkovski begin testimoni last week tell court object way run normal busi present work crimin fiction charg see support polit motiv part drive russian presid vladimir putin rein countri superrich busi leader socal oligarch yuko present 275bn â£13bn tax demand russian author key yugansk divis auction part settl bill compani effort gain bankruptci protect bid win damag sale dismiss court texa', '[\\'A close associate of former Yukos boss Mikhail Khodorkovsky has told a court that fraud charges levelled against him are \"false\".\\', \\'Platon Lebedev has been on trial alongside Mr Khodorkovsky since June in a case centring around the privatisation of a fertiliser firm. The pair claim they are being punished by the authorities for the political ambitions of Mr Khodorkovsky. Mr Lebedev said there were \"absurd contradictions\" in the case. Opening his defence, he said he could not see the legal basis of the charges he faced, which also include allegations of tax evasion. \"To my embarrassment, I could not understand the file of complaints against me,\" he told a Moscow court. Mr Lebedev headed the Menatep group, the parent company of Yukos.\\', \\'Mr Lebedev and Mr Khodorkovsky, who each face a possible 10 year jail sentence if convicted, will be questioned by a judge over the next few days. Mr Khodorkovsky began his testimony last week, telling the court that he objected to the way that the \"running of a normal business has been presented as a work of criminal fiction\". The charges are seen by supporters as politically motivated and part of a drive by Russian President Vladimir Putin to rein in the country\\\\\\'s super-rich business leaders, the so-called oligarchs. Yukos has been presented with a $27.5bn (Â£13bn) tax demand by the Russian authorities and its key Yugansk division was auctioned off to part settle the bill. The company\\\\\\'s effort to gain bankruptcy protection in the US - in a bid to win damages for the sale - were dismissed by a court in Texas.\\']', 'DATASET\\\\business']\n"
     ]
    }
   ],
   "source": [
    "print(l[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**More to come. Stay tuned!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# import pandas as pd\n",
    "pickle.dump( l, open( \"JESAN_2.0.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1,\n",
       "  'bulk alway le expens way go product like',\n",
       "  'Bulk is always the less expensive way to go for products like these',\n",
       "  'Health & Beauty'],\n",
       " [2,\n",
       "  'well duracel happi',\n",
       "  'Well they are not Duracell but for the price i am happy.',\n",
       "  'Health & Beauty'],\n",
       " [3,\n",
       "  'seem work well name brand much well',\n",
       "  'Seem to work as well as name brand batteries at a much better price',\n",
       "  'Health & Beauty'],\n",
       " [4,\n",
       "  'long last',\n",
       "  'These batteries are very long lasting the price is great.',\n",
       "  'Health & Beauty'],\n",
       " [5,\n",
       "  'lot christma amazonbas cell havent notic differ brand name basic brand lot easy purchas arriv hous hand buy',\n",
       "  \"Bought a lot of batteries for Christmas and the AmazonBasics Cell have been good. I haven't noticed a difference between the brand name batteries and the Amazon Basic brand. Just a lot easier to purchase and have arrive at the house and have on hand. Will buy again.\",\n",
       "  'Health & Beauty'],\n",
       " [6,\n",
       "  'ive problam order past plea',\n",
       "  'ive not had any problame with these batteries have ordered them in the past been very pleased.',\n",
       "  'Health & Beauty'],\n",
       " [7,\n",
       "  'well look cheap nonrecharg last quit perfect noth say',\n",
       "  'Well if you are looking for cheap non-rechargeable batteries that last quite a while then these are perfect. Nothing more to say.',\n",
       "  'Health & Beauty'],\n",
       " [8,\n",
       "  'hold amount high power juic like energ duracel half',\n",
       "  'These do not hold the amount of high power juice like energizer or duracell, but they are half the price.',\n",
       "  'Health & Beauty'],\n",
       " [9,\n",
       "  'amazonbas aa aaa do well appear shelf life ill buy',\n",
       "  \"AmazonBasics AA AAA batteries have done well by me appear to have a good shelf life. I'll buy them again.\",\n",
       "  'Health & Beauty'],\n",
       " [10,\n",
       "  'find basic equal superior name brand one cant believ didnt start buy sooner packag larg',\n",
       "  \"I find amazon basics batteries to be equal if not superior to name brand ones. Can't believe I didn't start buying them sooner! The packages are large and the price is great too.\",\n",
       "  'Health & Beauty'],\n",
       " [11,\n",
       "  'first start get basic realli like recent purchas seem last like mayb mixedbag inconsist last good other do test feel brand may last long howev hard beat',\n",
       "  'When I first started getting the Amazon basic batteries I really liked them. With recent purchases, they do not seem to last like they had, or maybe a mixed-bag (inconsistent with some lasting better than others). I have not done any tests, but feel some other brands may last longer. However, the price is hard to beat.',\n",
       "  'Health & Beauty'],\n",
       " [12,\n",
       "  'fish tank light night work easili switch want guest',\n",
       "  \"Use it for my fish tank's light at night and works great, I love how you can easily switch it off and on if you want it on while guests are there.\",\n",
       "  'Health & Beauty'],\n",
       " [13,\n",
       "  'get em cant realli comment job quick deliveri put two one keyboard go year say three day',\n",
       "  \"just got em so I can't really comment on how good the do the job, good price, quick delivery but have only put two into one of my keyboards but they can go up to a year so who can say after three days\",\n",
       "  'Health & Beauty'],\n",
       " [14,\n",
       "  'mani thing need aa batteri',\n",
       "  'we have many things that need aa battery they are great',\n",
       "  'Health & Beauty'],\n",
       " [15,\n",
       "  'thank abl find even good ship arriv perfect condit exactli need purchas would purchas',\n",
       "  'Thankful that I was able to find on Amazon for a great price and even better shipping. Arrived in perfect condition and did exactly what I needed it to. Great purchase and would purchase again.',\n",
       "  'Health & Beauty'],\n",
       " [16,\n",
       "  'dont know would buy thu brand seem like dont last long duracel',\n",
       "  \"I don't know if I would buy thus brand again seems like they don't last as long as Duracell\",\n",
       "  'Health & Beauty'],\n",
       " [17,\n",
       "  'opinion last anywher near long duracel thing like lead candl crazi trail camera camera expo cold temp le buy bulk north hous basic thing like sheet be towel opinion batteri life larg packag aaa aa size purchas lack',\n",
       "  'In my opinion these did not last anywhere near as long as Duracel in things like LED candles (which is crazy) and trail cameras. Cameras were not exposed to cold temps more or less than other batteries. WE buy in bulk for the north house. Amazon Basics is great for things like sheets and beeding and towels. In my opinion the battery life, in the large package of aaa and aa size we purchased were lacking.',\n",
       "  'Health & Beauty'],\n",
       " [18,\n",
       "  'dont last long brand name enough consid much cheap',\n",
       "  \"They don't last as long as the brand name but are good enough considering they are much cheaper.\",\n",
       "  'Health & Beauty'],\n",
       " [19,\n",
       "  'christma gift month decemb last like 2month toy need replac also use doorbel need replac tv remot control still work dont last long',\n",
       "  \"Bought these batteries for my Christmas gifts the month of (december) only lasted like 2months toys now need replacement batteries . I also used some for my doorbell and just now needs replacement batteries. Tv Remote control is still working but these batteries don't last very long...\",\n",
       "  'Health & Beauty'],\n",
       " [20,\n",
       "  'second order seem work name brand ship door',\n",
       "  'This my second order and they seem to work as good as name brand and ship to my door.',\n",
       "  'Health & Beauty'],\n",
       " [21,\n",
       "  'second purchas work even well name brand half way purchas',\n",
       "  'This was my second purchase of amazon batteries and they work great. Just as good or even better than name brand batteries and half the price. This will be the only way I purchase batteries from now on!',\n",
       "  'Health & Beauty'],\n",
       " [22,\n",
       "  'last long duracel xbox one control none explod like review say',\n",
       "  'They last as long as Duracell batteries in my Xbox one controllers and none of them exploded like some of the reviews said.',\n",
       "  'Health & Beauty'],\n",
       " [23,\n",
       "  'seem last long name brand name disappoint',\n",
       "  'they seemed to not last as long as other name brand names disappointed!',\n",
       "  'Health & Beauty'],\n",
       " [24,\n",
       "  'last long cheap happi',\n",
       "  'These do not last long at all very cheap batteries no happy',\n",
       "  'Health & Beauty'],\n",
       " [25,\n",
       "  'use yet batteri batteri',\n",
       "  'have not used yet but a battery is a battery good price',\n",
       "  'Health & Beauty'],\n",
       " [26,\n",
       "  'job although give 4star would say hand full strong pretti weak box 48 definit buy priceim pretti well satisfiedthank',\n",
       "  'These Amazon batteries did the job although I gave 4star only because I had a few I would say a hand full of batteries that were not as strong or were pretty weak but out of a box of 48 batteries, I will definitely buy again for this priceIm pretty well satisfied.Thank you!',\n",
       "  'Health & Beauty'],\n",
       " [27,\n",
       "  'weve order time caus weve happi product qualiti also happi would recommend',\n",
       "  \"We've ordered these a few times cause we've been so happy with product. These are good quality. We are also happy with price. I would recommend these.\",\n",
       "  'Health & Beauty'],\n",
       " [28,\n",
       "  'chanc hope remain order reason amount time mean year',\n",
       "  'HAVE NOT HAD CHANCE TO USE ALL OF THEM BUT HOPEFULLY ALL OF THEM WILL REMAIN IN GOOD ORDER FOR A REASONABLE AMOUNT OF TIME MEANING A FEW YEARS OUT.',\n",
       "  'Health & Beauty'],\n",
       " [29,\n",
       "  'light thought fit light arriv nice compani need aa okit fault know thank',\n",
       "  'these were under a light we thought they were there to fit the light when they arrived and nice company they were not the batteries we needed it should have been aa but that is ok,it was our fault not knowing this. thanks..',\n",
       "  'Health & Beauty'],\n",
       " [30,\n",
       "  'game camera work last sever week',\n",
       "  'I use these batteries in my game cameras and they work great, last several weeks.',\n",
       "  'Health & Beauty'],\n",
       " [31,\n",
       "  'conveni box long need compar price store do long time ago',\n",
       "  'Very convenient to have a box of batteries and at a good price. No longer need to compare pricing in stores. Should have done this long time ago.',\n",
       "  'Health & Beauty'],\n",
       " [32,\n",
       "  'much say except work hundr let share kid grand kid',\n",
       "  \"Not much to say about batteries except they work for a good price. Having a hundred let's me share with kids and grand kids.\",\n",
       "  'Health & Beauty'],\n",
       " [33,\n",
       "  'seem work okay far',\n",
       "  'They seem to work okay so far and the price was great.',\n",
       "  'Health & Beauty']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unpickled_data = pd.read_pickle(\"Amazon_full.p\")\n",
    "unpickled_data[1:34]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = numpy.array(unpickled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.DataFrame(arr) \n",
    "dataframe.to_csv(\"data11.csv\")\n",
    "nlinesfile =28300\n",
    "nlinesrandomsample = 5000\n",
    "lines2skip = numpy.random.choice(numpy.arange(1,nlinesfile+1), (nlinesfile-nlinesrandomsample), replace=False)\n",
    "#t = pd.read_csv(filename, sep=';',encoding='utf-8', skiprows=lines2skip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "add=pd.read_csv(\"data11.csv\",encoding='utf-8', skiprows=lines2skip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>mani thing need aa batteri</td>\n",
       "      <td>we have many things that need aa battery they ...</td>\n",
       "      <td>Health &amp; Beauty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>thank abl find even good ship arriv perfect co...</td>\n",
       "      <td>Thankful that I was able to find on Amazon for...</td>\n",
       "      <td>Health &amp; Beauty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>opinion last anywher near long duracel thing l...</td>\n",
       "      <td>In my opinion these did not last anywhere near...</td>\n",
       "      <td>Health &amp; Beauty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>job although give 4star would say hand full st...</td>\n",
       "      <td>These Amazon batteries did the job although I ...</td>\n",
       "      <td>Health &amp; Beauty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>light thought fit light arriv nice compani nee...</td>\n",
       "      <td>these were under a light we thought they were ...</td>\n",
       "      <td>Health &amp; Beauty</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0   0                                                  1  \\\n",
       "0          14  14                         mani thing need aa batteri   \n",
       "1          15  15  thank abl find even good ship arriv perfect co...   \n",
       "2          17  17  opinion last anywher near long duracel thing l...   \n",
       "3          26  26  job although give 4star would say hand full st...   \n",
       "4          29  29  light thought fit light arriv nice compani nee...   \n",
       "\n",
       "                                                   2                3  \n",
       "0  we have many things that need aa battery they ...  Health & Beauty  \n",
       "1  Thankful that I was able to find on Amazon for...  Health & Beauty  \n",
       "2  In my opinion these did not last anywhere near...  Health & Beauty  \n",
       "3  These Amazon batteries did the job although I ...  Health & Beauty  \n",
       "4  these were under a light we thought they were ...  Health & Beauty  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
